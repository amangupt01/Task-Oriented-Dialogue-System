{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines, argparse, random, os, sys\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from transformers import TrainingArguments\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, set_seed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'train_path': 'data/train.jsonl',\n",
    "    'dev_path': 'data/dev.jsonl',\n",
    "    'model': 'gpt2',\n",
    "    'model_path': 'cs1190444_cs1190673_model',\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'init_lr': 1e-5,\n",
    "    'use_random_split': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TODDataset(Dataset):\n",
    "    def __init__(self, data_paths, tokenizer, aux_input=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attention_mask = []\n",
    "        for data_path in data_paths:\n",
    "            print('Loading data from {}'.format(data_path))\n",
    "            with jsonlines.open(data_path) as reader:\n",
    "                for obj in reader:\n",
    "                    input_raw = self.get_input(obj) if not aux_input else self.get_aux_input(obj)\n",
    "                    output_raw = self.get_output(obj)\n",
    "                    text = \"<bos> \" + input_raw.strip() + \" <sep> \" + output_raw.strip() + \" <eos>\"\n",
    "                    encoded_input = tokenizer(text, return_tensors='pt', return_attention_mask=True)\n",
    "                    self.input_ids.append(encoded_input['input_ids'][0])\n",
    "                    self.attention_mask.append(encoded_input['attention_mask'][0])\n",
    "        print(\"Number of examples: {}\".format(len(self.input_ids))) \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.input_ids[idx], \"attention_mask\": self.attention_mask[idx]}\n",
    "    \n",
    "    def get_input(self, obj):\n",
    "        input_str = obj['input']\n",
    "        return input_str\n",
    "\n",
    "    def get_aux_input(self, obj):\n",
    "        input_str = obj['input']\n",
    "        history_str = ' '.join([x['user_query'] + ' ' + x['response_text'] for x in obj['history']])\n",
    "        user_lists_str = ' '.join([l['name'] + ' ' + ' '.join(l['items']) for l in obj['user_lists']])\n",
    "        user_notes_str = ' '.join([n['name'] + ' ' + n['content'] for n in obj['user_notes']])\n",
    "        user_contacts_str = ' '.join(obj['user_contacts'])\n",
    "        return input_str + ' ' + history_str + ' ' + user_lists_str + ' ' + user_notes_str + ' ' + user_contacts_str\n",
    "\n",
    "    def get_output(self, obj):\n",
    "        output_str = obj['output']\n",
    "        return output_str\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    # Group samples by length\n",
    "    batch = sorted(batch, key=lambda x: x['input_ids'].size(0))\n",
    "    \n",
    "    # Get input_ids and attention_mask tensors\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([x['input_ids'] for x in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence([x['attention_mask'] for x in batch], batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, optimizer, tokenizer, train_data):\n",
    "    num_batches = len(train_data)\n",
    "    running_loss = 0\n",
    "    with tqdm(total=num_batches, desc=\"Training\", unit=\"batch\", leave=False) as pbar:\n",
    "        for _, batch in enumerate(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask = attention_mask, labels=input_ids)\n",
    "            loss = outputs[0]\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.update(1)\n",
    "        print(f\"Training loss: {running_loss / num_batches:.4f}\")\n",
    "        torch.save(model.state_dict(), args['model_path'])\n",
    "\n",
    "def train(model, tokenizer, optimizer, train_data, dev_data, args):\n",
    "    model.train()\n",
    "    print(\"Training started ...\")\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        train_epoch(epoch, model, optimizer, tokenizer, train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_path': 'data/train.jsonl', 'dev_path': 'data/dev.jsonl', 'model': 'gpt2', 'model_path': 'cs1190444_cs1190673_model', 'num_epochs': 10, 'batch_size': 32, 'init_lr': 1e-05, 'use_random_split': False}\n",
      "Loading data from data/train.jsonl\n",
      "Number of examples: 30993\n",
      "Loading data from data/dev.jsonl\n",
      "Number of examples: 9272\n",
      "Loaded Train Dataset with 30993 batches\n",
      "Loaded Dev Dataset with 9272 batches\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(args)\n",
    "\n",
    "if (args['model'].startswith('gpt2')):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>', \n",
    "                                    'bos_token': '<bos>',\n",
    "                                    'eos_token': '<eos>'})\n",
    "    tokenizer.add_tokens(['<sep>'])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(device)\n",
    "if (not args['use_random_split']):\n",
    "    train_dataset = TODDataset([args['train_path']], tokenizer)\n",
    "    dev_dataset = TODDataset([args['dev_path']], tokenizer)\n",
    "\n",
    "train_data = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "print(\"Loaded Train Dataset with {} batches\".format(len(train_dataset)))\n",
    "dev_data = DataLoader(dev_dataset, batch_size=args['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "print(\"Loaded Dev Dataset with {} batches\".format(len(dev_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|█████████▉| 968/969 [02:28<00:00,  6.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.3956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|█████████▉| 968/969 [02:28<00:00,  7.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|█████████▉| 968/969 [02:29<00:00,  6.54batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 969/969 [02:29<00:00,  6.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|█████████▉| 968/969 [02:29<00:00,  6.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|█████████▉| 968/969 [02:29<00:00,  5.91batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|█████████▉| 968/969 [02:28<00:00,  6.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|█████████▉| 968/969 [02:29<00:00,  6.71batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|█████████▉| 968/969 [02:28<00:00,  6.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|█████████▉| 968/969 [02:28<00:00,  6.69batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args['init_lr'])\n",
    "train(model, tokenizer, optimizer, train_data, dev_data, args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change my Facebook status to say to say time for a road trip! <sep> Post_message ( message « time for a road trip » provider « Facebook » )\n",
      "Change my Facebook status to say to say time for a road trip! <sep> Post_message ( provider « Facebook » )\n",
      "Change my Facebook status to say to say time for a road trip! <sep> Post_message ( message « time for a road trip » provider « Facebook » )\n",
      "Change my Facebook status to say to say time for a road trip! <sep> Post_message ( message « to say time for a road trip! » provider « Facebook » )\n",
      "Change my Facebook status to say to say time for a road trip! <sep> Post_message ( message « time for a road trip » provider « Facebook » )\n"
     ]
    }
   ],
   "source": [
    "test = \"Change my Facebook status to say to say time for a road trip!\"\n",
    "test = \"<bos> \" + test.strip() + \" <sep> \"\n",
    "encoded_input = tokenizer(test, return_tensors='pt', return_attention_mask=True)\n",
    "input_ids = encoded_input['input_ids'].to(device)\n",
    "attention_mask = encoded_input['attention_mask'].to(device)\n",
    "outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=40, do_sample=True,top_p=0.95, num_return_sequences=5)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
